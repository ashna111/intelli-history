{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shivani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shivani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.codementor.io/hirenpatel545/5-best-javascript-web-scraping-libraries-and-tools-sicow2rx9\"\n",
    "headers = {'User-Agent':'Mozilla/5.0'}\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Best JavaScript Web Scraping Libraries and Tools | Codementor\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('title').text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introduction\n",
      " 1. Request (Aka HTTP Client) :\n",
      " 2. Cheerio (Aka Parser) :\n",
      " 3. Osmosis (Aka Parser) :\n",
      " 4. Puppeteer (Aka Headless Chrome Browser for Automation) :\n",
      " 5. Apify SDK (Aka The Complete Web Scraping Framework) :\n",
      " Conclusion\n"
     ]
    }
   ],
   "source": [
    "all_headings = soup.find_all(\"h2\")\n",
    "# print(all_headings)\n",
    "headings = []\n",
    "for heading in all_headings:\n",
    "    headings.append(heading.text)\n",
    "    print(heading.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As it’s obvious, Internet is getting overloaded with information and data. With the growth of data on the web, web scraping is also likely to become more and more important for businesses for mining the Internet for actionable insights. While there are various tools available for web scraping, a growing number of people spend their valuable time exploring web scraping libraries and tools for JavaScript.\n"
     ]
    }
   ],
   "source": [
    "for x in all_paragraphs:\n",
    "    content.append(x.get_text())\n",
    "\n",
    "print(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSummary(content):\n",
    "    tokens = word_tokenize(content.lower())\n",
    "    sentences = sent_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = [word for word in tokens if (word.isalpha() and word not in stop_words)]\n",
    "#     words[:10]\n",
    "    word_freq = FreqDist(words)\n",
    "    rankings = defaultdict(int)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                rankings[i] += word_freq[word]\n",
    "#      rankings    \n",
    "    indexes = nlargest(3, rankings, key=rankings.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As it’s obvious, Internet is getting overloaded with information and data. With the growth of data on the web, web scraping is also likely to become more and more important for businesses for mining the Internet for actionable insights. While there are various tools available for web scraping, a growing number of people spend their valuable time exploring web scraping libraries and tools for JavaScript.\n"
     ]
    }
   ],
   "source": [
    "for para in all_paragraphs:\n",
    "    summary_individual = findSummary(para.get_text())\n",
    "    summary.append(summary_individual)\n",
    "\n",
    "print(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJSON(content,headings):\n",
    "    print(\"Creating JSON object for each url\")\n",
    "    lang = \"en\"\n",
    "    sentences = []\n",
    "    tokens_sentences = []\n",
    "    POS_tokens = []\n",
    "    tokens = []\n",
    "    tokens_sentences_lemmatized = []\n",
    "    for c in content:\n",
    "        tokens.append(keywords(i).split('\\n'))\n",
    "        sentences_per_para = sent_tokenize(c)\n",
    "        tokens_sentences_per_para = []\n",
    "        POS_tokens_per_para = []\n",
    "        tokens_sentences_lemmatized_per_para = []\n",
    "        for sentence in sent_tokenize(c):\n",
    "            sent_lem = []\n",
    "            words = word_tokenize(sentence)\n",
    "            tokens_sentences_per_para.append(words)\n",
    "            for word in words:\n",
    "                lemm = lemmatizer.lemmatize(word)\n",
    "                sent_lem.append(lemm)\n",
    "            tokens_sentences_lemmatized_per_para.append(sent_lem)\n",
    "            POS_tokens_per_para.append(nltk.pos_tag(words))\n",
    "        sentences.append(sentences_per_para)\n",
    "        tokens_sentences.append(tokens_sentences_per_para)\n",
    "        POS_tokens.append(POS_tokens_per_para)\n",
    "        tokens_sentences_lemmatized.append(tokens_sentences_lemmatized_per_para)\n",
    "        \n",
    "    dict = {\n",
    "        \"articles\": headings,\n",
    "        \"lang\":\"en\",\n",
    "        \"sentences\":sentences,\n",
    "        \"tokens_sentences\":tokens_sentences,\n",
    "        \"POS_tokens\":POS_tokens,\n",
    "        \"tokens\":tokens,\n",
    "        \"tokens_sentences_lemmatized\":tokens_sentences_lemmatized\n",
    "    }\n",
    "    \n",
    "    return dict;\n",
    "    \n",
    "#     JSON object creation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON object for each url\n",
      "Creating JSON object for each url\n",
      "Creating JSON object for each url\n"
     ]
    }
   ],
   "source": [
    "urls = [ \"https://www.codementor.io/hirenpatel545/5-best-javascript-web-scraping-libraries-and-tools-sicow2rx9\",\"https://medium.com/ninjaconcept/interactive-dynamic-force-directed-graphs-with-d3-da720c6d7811\",\"https://en.wikipedia.org/wiki/Alphabet_Inc.\"]\n",
    "data = []\n",
    "for url in urls:\n",
    "    headers = {'User-Agent':'Mozilla/5.0'}\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    all_headings = soup.find_all(\"h2\")\n",
    "    headings = []\n",
    "    for heading in all_headings:\n",
    "        headings.append(heading.text)\n",
    "    \n",
    "    dict = createJSON(url,headings)\n",
    "    data.append(dict)\n",
    "\n",
    "json_data = {\"para\":data}  \n",
    "json_string = json.dumps(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapJSONtoCSV(json_data):\n",
    "    json_parsed = json.loads(json_data)\n",
    "    j_data = json_parsed['para']\n",
    "    csv_data = open('Data.csv', 'w')\n",
    "    csvwriter = csv.writer(csv_data)\n",
    "    count = 0\n",
    "    for d in j_data:\n",
    "        if count == 0:\n",
    "            header = d.keys()\n",
    "            csvwriter.writerow(header)\n",
    "            count += 1\n",
    "            csvwriter.writerow(d.values())\n",
    "            csv_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(mapJSONtoCSV(json_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
